
[DEFAULTS]

#name of the run
name = enfusion

# dataset data
dataset_path = 'artefacts/alv_full_dataset.pt'

encodec_frames_per_second = 150

# total n seconds of audio in each batch
# used to determine batch size
batch_seconds = 4096

; num_channels = 128
; # Number of audio samples for the training input
; sample_size = 150

# number of GPUs to use for training
num_gpus = 1 

loss="z_mse" #"z_mse"

lr = 4e-4

; loss="mssl"
; batch_size = 64
; lr = 4e-4

# number of nodes to use for training
num_nodes = 1 

# number of CPU workers for the DataLoader
num_workers = 2

# Number of steps between demos
demo_every = 1000

# Number of denoising steps for the demos       
demo_steps = 250

# Number of demos to create
num_demos = 4

# the EMA decay
ema_decay = 0.995       

# the random seed
seed = 42

# Batches for gradient accumulation
accum_batches = 1

# The sample rate of the audio
sample_rate = 48000   

# Number of steps between checkpoints
checkpoint_every = 10000                              

# unused, required by the model code
latent_dim = 0              

# If true training data is kept in RAM
cache_training_data = False  


# checkpoint file to (re)start training from
ckpt_path = ''

# Path to output the model checkpoints
save_path = ''

#the multiprocessing start method ['fork', 'forkserver', 'spawn']
start_method = 'spawn'

# Whether to save the model checkpoints to Weights & Biases
save_wandb = 'none'
