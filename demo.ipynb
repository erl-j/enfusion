{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "cellView": "form",
                "id": "haxvUGZ0VpzA"
            },
            "outputs": [],
            "source": [
                "#@title Imports and definitions\n",
                "from prefigure.prefigure import get_all_args\n",
                "from contextlib import contextmanager\n",
                "from copy import deepcopy\n",
                "import math\n",
                "from pathlib import Path\n",
                "from google.colab import files\n",
                "\n",
                "import os, signal, sys\n",
                "import gc\n",
                "\n",
                "from diffusion import sampling\n",
                "import torch\n",
                "from torch import optim, nn\n",
                "from torch.nn import functional as F\n",
                "from torch.utils import data\n",
                "from tqdm import trange\n",
                "from einops import rearrange\n",
                "\n",
                "import torchaudio\n",
                "from audio_diffusion.models import DiffusionAttnUnet1D\n",
                "import numpy as np\n",
                "\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "import IPython.display as ipd\n",
                "from audio_diffusion.utils import Stereo, PadCrop\n",
                "from glob import glob\n",
                "\n",
                "#@title Model code\n",
                "class DiffusionUncond(nn.Module):\n",
                "    def __init__(self, global_args):\n",
                "        super().__init__()\n",
                "\n",
                "        self.diffusion = DiffusionAttnUnet1D(global_args, n_attn_layers = 4)\n",
                "        self.diffusion_ema = deepcopy(self.diffusion)\n",
                "        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import IPython.display as ipd\n",
                "\n",
                "def plot_and_hear(audio, sr):\n",
                "    display(ipd.Audio(audio.cpu().clamp(-1, 1), rate=sr))\n",
                "    plt.plot(audio.cpu().t().numpy())\n",
                "  \n",
                "def load_to_device(path, sr):\n",
                "    audio, file_sr = torchaudio.load(path)\n",
                "    if sr != file_sr:\n",
                "      audio = torchaudio.transforms.Resample(file_sr, sr)(audio)\n",
                "    audio = audio.to(device)\n",
                "    return audio\n",
                "\n",
                "def get_alphas_sigmas(t):\n",
                "    \"\"\"Returns the scaling factors for the clean image (alpha) and for the\n",
                "    noise (sigma), given a timestep.\"\"\"\n",
                "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
                "\n",
                "def get_crash_schedule(t):\n",
                "    sigma = torch.sin(t * math.pi / 2) ** 2\n",
                "    alpha = (1 - sigma ** 2) ** 0.5\n",
                "    return alpha_sigma_to_t(alpha, sigma)\n",
                "\n",
                "def t_to_alpha_sigma(t):\n",
                "    \"\"\"Returns the scaling factors for the clean image and for the noise, given\n",
                "    a timestep.\"\"\"\n",
                "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
                "\n",
                "def alpha_sigma_to_t(alpha, sigma):\n",
                "    \"\"\"Returns a timestep, given the scaling factors for the clean image and for\n",
                "    the noise.\"\"\"\n",
                "    return torch.atan2(sigma, alpha) / math.pi * 2\n",
                "\n",
                "#@title Args\n",
                "sample_size = 65536 \n",
                "sample_rate = 48000   \n",
                "latent_dim = 0             \n",
                "\n",
                "class Object(object):\n",
                "    pass\n",
                "\n",
                "args = Object()\n",
                "args.sample_size = sample_size\n",
                "args.sample_rate = sample_rate\n",
                "args.latent_dim = latent_dim\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "SMQ8vYNQO22Y"
            },
            "source": [
                "# Model settings"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LWxBqHH_Yjvt"
            },
            "source": [
                "Select the model you want to sample from\n",
                "---\n",
                "Model name | Description | Sample rate | Output samples\n",
                "--- | --- | --- | ---\n",
                "glitch-440k |Trained on clips from samples provided by [glitch.cool](https://glitch.cool) | 48000 | 65536\n",
                "jmann-small-190k |Trained on clips from Jonathan Mann's [Song-A-Day](https://songaday.world/) project | 48000 | 65536\n",
                "jmann-large-580k |Trained on clips from Jonathan Mann's [Song-A-Day](https://songaday.world/) project | 48000 | 131072\n",
                "maestro-150k |Trained on piano clips from the [MAESTRO](https://magenta.tensorflow.org/datasets/maestro) dataset | 16000 | 65536\n",
                "unlocked-250k |Trained on clips from the [Unlocked Recordings](https://archive.org/details/unlockedrecordings) dataset | 16000 | 65536\n",
                "honk-140k |Trained on recordings of the Canada Goose from [xeno-canto](https://xeno-canto.org/) | 16000 | 65536\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "cellView": "form",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "JHsHQcc6rHu7",
                "outputId": "12ba8578-eda5-4142-8f0f-8c67b1f6b3a6"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "maestro-150k already downloaded. If the file is corrupt, enable check_model_SHA.\n",
                        "Creating the model...\n",
                        "Model created\n"
                    ]
                }
            ],
            "source": [
                "from urllib.parse import urlparse\n",
                "import hashlib\n",
                "import k_diffusion as K\n",
                "\n",
                "\n",
                "custom_ckpt_path = ''#@param {type: 'string'}\n",
                "\n",
                "custom_sample_rate = 16000 #@param {type: 'number'}\n",
                "custom_sample_size = 65536 #@param {type: 'number'}\n",
                "\n",
                "ckpt_path = custom_ckpt_path\n",
                "args.sample_size = custom_sample_size\n",
                "args.sample_rate = custom_sample_rate\n",
                "\n",
                "print(\"Creating the model...\")\n",
                "model = DiffusionUncond(args)\n",
                "model.load_state_dict(torch.load(ckpt_path)[\"state_dict\"])\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = model.requires_grad_(False).to(device)\n",
                "print(\"Model created\")\n",
                "\n",
                "# # Remove non-EMA\n",
                "del model.diffusion\n",
                "\n",
                "model_fn = model.diffusion_ema"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Knzr6CYmtaI_"
            },
            "source": [
                "Select the sampler you want to use\n",
                "---\n",
                "Sampler name | Notes\n",
                "--- | ---\n",
                "v-iplms | This is what the model expects. Needs more steps, but more reliable.\n",
                "k-heun | Needs fewer steps, but ideal sigma_min and sigma_max need to be found. Doesn't work with all models.\n",
                "k-dpmpp_2s_ancestral | Fastest sampler, but you may have to find new sigmas. Recommended min & max sigmas: 0.01, 80\n",
                "k-lms | \"\n",
                "k-dpm-2 | \"\n",
                "k-dpm-fast | \"\n",
                "k-dpm-adaptive | Takes in extra parameters for quality, step count is non-deterministic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "cellView": "form",
                "id": "iyR4w86-ei_z"
            },
            "outputs": [],
            "source": [
                "#@title Sampler options\n",
                "sampler_type = \"v-iplms\" #@param [\"v-iplms\", \"k-heun\", \"k-dpmpp_2s_ancestral\", \"k-lms\", \"k-dpm-2\", \"k-dpm-fast\", \"k-dpm-adaptive\"]\n",
                "\n",
                "#@markdown ---\n",
                "#@markdown **K-diffusion settings (advanced)**\n",
                "sigma_min = 0.0001 #@param {type: \"number\"}\n",
                "sigma_max = 1 #@param {type: \"number\"}\n",
                "rho=7. #@param {type: \"number\"}\n",
                "#@markdown k-dpm-adaptive settings\n",
                "rtol = 0.01 #@param {type: \"number\"}\n",
                "atol = 0.01 #@param {type: \"number\"}\n",
                "\n",
                "def sample(model_fn, noise, steps=100, sampler_type=\"v-iplms\", noise_level = 1.0):\n",
                "  #Check for k-diffusion\n",
                "  if sampler_type.startswith('k-'):\n",
                "    denoiser = K.external.VDenoiser(model_fn)\n",
                "    sigmas = K.sampling.get_sigmas_karras(steps, sigma_min, sigma_max, rho, device=device)\n",
                "\n",
                "  if sampler_type == \"v-iplms\":\n",
                "    t = torch.linspace(1, 0, steps + 1, device=device)[:-1]\n",
                "    step_list = get_crash_schedule(t)\n",
                "\n",
                "    return sampling.iplms_sample(model_fn, noise, step_list, {})\n",
                "\n",
                "  elif sampler_type == \"k-heun\":\n",
                "    return K.sampling.sample_heun(denoiser, noise, sigmas, disable=False)\n",
                "  elif sampler_type == \"k-lms\":\n",
                "    return K.sampling.sample_lms(denoiser, noise, sigmas, disable=False)\n",
                "  elif sampler_type == \"k-dpmpp_2s_ancestral\":\n",
                "    return K.sampling.sample_dpmpp_2s_ancestral(denoiser, noise, sigmas, disable=False)\n",
                "  elif sampler_type == \"k-dpm-2\":\n",
                "    return K.sampling.sample_dpm_2(denoiser, noise, sigmas, disable=False)\n",
                "  elif sampler_type == \"k-dpm-fast\":\n",
                "    return K.sampling.sample_dpm_fast(denoiser, noise, sigma_min, sigma_max, steps, disable=False)\n",
                "  elif sampler_type == \"k-dpm-adaptive\":\n",
                "    return K.sampling.sample_dpm_adaptive(denoiser, noise, sigma_min, sigma_max, rtol=rtol, atol=atol, disable=False)\n",
                "\n",
                "def resample(model_fn, audio, steps=100, sampler_type=\"v-iplms\", noise_level = 1.0):\n",
                "  #Noise the input\n",
                "  if sampler_type == \"v-iplms\":\n",
                "    t = torch.linspace(0, 1, steps + 1, device=device)\n",
                "    step_list = get_crash_schedule(t)\n",
                "    step_list = step_list[step_list < noise_level]\n",
                "\n",
                "    alpha, sigma = t_to_alpha_sigma(step_list[-1])\n",
                "    noised = torch.randn([batch_size, 2, effective_length], device='cuda')\n",
                "    noised = audio * alpha + noised * sigma\n",
                "\n",
                "  elif sampler_type.startswith(\"k-\"):\n",
                "    denoiser = K.external.VDenoiser(model_fn)\n",
                "    noised = audio + torch.randn_like(audio) * noise_level\n",
                "    sigmas = K.sampling.get_sigmas_karras(steps, sigma_min, noise_level, rho, device=device)\n",
                "\n",
                "  # Denoise\n",
                "  if sampler_type == \"v-iplms\":\n",
                "    return sampling.iplms_sample(model_fn, noised, step_list.flip(0)[:-1], {})\n",
                "\n",
                "  elif sampler_type == \"k-heun\":\n",
                "    return K.sampling.sample_heun(denoiser, noised, sigmas, disable=False)\n",
                "\n",
                "  elif sampler_type == \"k-dpmpp_2s_ancestral\":\n",
                "    return K.sampling.sample_dpmpp_2s_ancestral(denoiser, noised, sigmas, disable=False)\n",
                "\n",
                "  elif sampler_type == \"k-lms\":\n",
                "    return K.sampling.sample_lms(denoiser, noised, sigmas, disable=False)\n",
                "\n",
                "  elif sampler_type == \"k-dpm-2\":\n",
                "    return K.sampling.sample_dpm_2(denoiser, noised, sigmas, s_noise=0., disable=False)\n",
                "\n",
                "  elif sampler_type == \"k-dpm-fast\":\n",
                "    return K.sampling.sample_dpm_fast(denoiser, noised, sigma_min, noise_level, steps, disable=False)\n",
                "\n",
                "  elif sampler_type == \"k-dpm-adaptive\":\n",
                "    return K.sampling.sample_dpm_adaptive(denoiser, noised, sigma_min, noise_level, rtol=rtol, atol=atol, disable=False)\n",
                "\n",
                "def reverse_sample(model_fn, audio, steps=100, sampler_type=\"v-iplms\", noise_level = 1.0):\n",
                "  \n",
                "  if sampler_type == \"v-iplms\":\n",
                "    t = torch.linspace(0, 1, steps + 1, device=device)\n",
                "    step_list = get_crash_schedule(t)\n",
                "\n",
                "    return sampling.iplms_sample(model_fn, audio_samples, step_list, {}, is_reverse=True)\n",
                "\n",
                "  elif sampler_type.startswith(\"k-\"):\n",
                "    denoiser = K.external.VDenoiser(model_fn)\n",
                "    sigmas = K.sampling.get_sigmas_karras(steps, sigma_min, noise_level, rho, device=device)\n",
                "\n",
                "  # Denoise\n",
                "  if sampler_type == \"k-heun\":\n",
                "    return K.sampling.sample_heun(denoiser, audio, sigmas.flip(0)[:-1], disable=False)\n",
                "  elif sampler_type == \"k-lms\":\n",
                "    return K.sampling.sample_lms(denoiser, audio, sigmas.flip(0)[:-1], disable=False)\n",
                "  elif sampler_type == \"k-dpmpp_2s_ancestral\":\n",
                "    return K.sampling.sample_dpmpp_2s_ancestral(denoiser, audio, sigmas.flip(0)[:-1], disable=False)\n",
                "  elif sampler_type == \"k-dpm-2\":\n",
                "    return K.sampling.sample_dpm_2(denoiser, audio, sigmas.flip(0)[:-1], s_noise=0., disable=False)\n",
                "  elif sampler_type == \"k-dpm-fast\":\n",
                "    return K.sampling.sample_dpm_fast(denoiser, audio, noise_level, sigma_min, steps, disable=False)\n",
                "\n",
                "  elif sampler_type == \"k-dpm-adaptive\":\n",
                "    return K.sampling.sample_dpm_adaptive(denoiser, audio, noise_level, sigma_min, rtol=rtol, atol=atol, disable=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_GQK9yZHTr_z"
            },
            "source": [
                "# Generate new sounds\n",
                "\n",
                "Feeding white noise into the model to be denoised creates novel sounds in the \"space\" of the training data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "zntGqLTJq6xU"
            },
            "outputs": [],
            "source": [
                "#@markdown How many audio clips to create\n",
                "batch_size =  4#@param {type:\"number\"}\n",
                "\n",
                "#@markdown Number of steps (100 is a good start, more steps trades off speed for quality)\n",
                "steps = 100 #@param {type:\"number\"}\n",
                "\n",
                "#@markdown Check the box below to skip this section when running all cells\n",
                "skip_for_run_all = False #@param {type: \"boolean\"}\n",
                "\n",
                "\n",
                "if not skip_for_run_all:\n",
                "  torch.cuda.empty_cache()\n",
                "  gc.collect()\n",
                "\n",
                "  # Generate random noise to sample from\n",
                "  noise = torch.randn([batch_size, 2, effective_length]).to(device)\n",
                "\n",
                "  generated = sample(model_fn, noise, steps, sampler_type)\n",
                "\n",
                "  # Hard-clip the generated audio\n",
                "  generated = generated.clamp(-1, 1)\n",
                "\n",
                "  # Put the demos together\n",
                "  generated_all = rearrange(generated, 'b d n -> d (b n)')\n",
                "\n",
                "  print(\"All samples\")\n",
                "  plot_and_hear(generated_all, args.sample_rate)\n",
                "  for ix, gen_sample in enumerate(generated):\n",
                "    print(f'sample #{ix + 1}')\n",
                "    display(ipd.Audio(gen_sample.cpu(), rate=args.sample_rate))\n",
                "\n",
                "\n",
                "else:\n",
                "  print(\"Skipping section, uncheck 'skip_for_run_all' to enable\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "v0WKP7ku67vn"
            },
            "source": [
                "# Regenerate your own sounds\n",
                "By adding noise to an audio file and running it through the model to be denoised, new details will be created, pulling the audio closer to the \"sonic space\" of the model. The more noise you add, the more the sound will change.\n",
                "\n",
                "The effect of this is a kind of \"style transfer\" on the audio. For those familiar with image generation models, this is analogous to an \"init image\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "oTK_JxQ0arky"
            },
            "outputs": [],
            "source": [
                "#@title Record audio or enter a filepath to a prerecorded audio file\n",
                "import torch\n",
                "import torchaudio\n",
                "from typing import Iterable, Tuple\n",
                "\n",
                "Audio = Tuple[int, np.ndarray]\n",
                "\n",
                "#@markdown Check the box below to create an audio recording interface below\n",
                "record_audio = True #@param {type: \"boolean\"}\n",
                "\n",
                "#@markdown If you left \"record_audio\" blank, enter a path to an audio file you want to alter, or leave blank to upload a file (.wav or .flac).\n",
                "file_path = \"\" #@param{type:\"string\"}\n",
                "\n",
                "#@markdown Number of audio recordings to combine into one clip. Only applies if the \"record_audio\" box is checked.\n",
                "n_audio_recordings = 1 #@param{type:\"number\"}\n",
                "\n",
                "# this is a global variable to be filled in by the generate_from_audio callback\n",
                "recording_file_path = \"\"\n",
                "\n",
                "\n",
                "def combine_audio(*audio_iterable: Iterable[Audio]) -> Audio:\n",
                "    \"\"\"Combines an iterable of audio signals into one.\"\"\"\n",
                "    max_len = max([x.shape for _, x in audio_iterable])\n",
                "    combined_audio = np.zeros(max_len, dtype=np.int32)\n",
                "    for _, a in audio_iterable:\n",
                "        combined_audio[:a.shape[0]] = combined_audio[:a.shape[0]] * .5 + a * .5\n",
                "    return combined_audio\n",
                "\n",
                "\n",
                "def generate_from_audio(file_path: str, *audio_iterable: Iterable[Audio]):\n",
                "    sample_rate = audio_iterable[0][0]\n",
                "    combined_audio = combine_audio(*audio_iterable)\n",
                "    tensor = torch.from_numpy(\n",
                "        np.concatenate(\n",
                "            [\n",
                "                combined_audio.reshape(1, -1),\n",
                "                combined_audio.reshape(1, -1)\n",
                "            ],\n",
                "            axis=0,\n",
                "        )\n",
                "    )\n",
                "    global recording_file_path\n",
                "    recording_file_path = file_path\n",
                "    torchaudio.save(\n",
                "        file_path,\n",
                "        tensor,\n",
                "        sample_rate=sample_rate,\n",
                "        format=\"wav\"\n",
                "    )\n",
                "    return (sample_rate, combined_audio), file_path\n",
                "\n",
                "if record_audio:\n",
                "    recording_interface = gr.Interface(\n",
                "        fn=generate_from_audio,\n",
                "        inputs=[\n",
                "            gr.Textbox(\n",
                "                \"/content/recording.wav\",\n",
                "                label=\"save recording to filepath\",\n",
                "            ),\n",
                "            *[\n",
                "                gr.Audio(source=\"microphone\", label=f\"audio clip {i}\")\n",
                "                for i in range(1, n_audio_recordings + 1)\n",
                "            ]\n",
                "        ],\n",
                "        outputs=[\n",
                "            gr.Audio(label=\"combined output audio\"),\n",
                "            gr.File(label=\"output file\"),\n",
                "        ],\n",
                "        allow_flagging=\"never\",\n",
                "    )\n",
                "\n",
                "    recording_interface.launch();\n",
                "elif file_path == \"\":\n",
                "    print(\"No file path provided, please upload a file\")\n",
                "    # uploaded = files.upload()\n",
                "    file_path = list(uploaded.keys())[0]\n",
                "\n",
                "if not record_audio:\n",
                "    print(f\"Using file_path: {file_path} to regenerate new sounds.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "bKgS7vZc4lN9"
            },
            "outputs": [],
            "source": [
                "#@title Generate new sounds from recording\n",
                "\n",
                "#@markdown Total number of steps (100 is a good start, more steps trades off speed for quality)\n",
                "steps = 100#@param {type:\"number\"}\n",
                "\n",
                "#@markdown How much (0-1) to re-noise the original sample. Adding more noise (a higher number) means a bigger change to the input audio\n",
                "noise_level = 0.3#@param {type:\"number\"}\n",
                "\n",
                "#@markdown Multiplier on the default sample length from the model, allows for longer audio clips at the expense of VRAM\n",
                "sample_length_mult = 2#@param {type:\"number\"}\n",
                "\n",
                "#@markdown How many variations to create\n",
                "batch_size = 4 #@param {type:\"number\"}\n",
                "\n",
                "#@markdown Check the box below to save your generated audio to [Weights & Biases](https://www.wandb.ai/site)\n",
                "save_own_generations_to_wandb = False #@param {type: \"boolean\"}\n",
                "\n",
                "#@markdown Check the box below to skip this section when running all cells\n",
                "skip_for_run_all = False #@param {type: \"boolean\"}\n",
                "\n",
                "effective_length = args.sample_size * sample_length_mult\n",
                "\n",
                "if not skip_for_run_all:\n",
                "  torch.cuda.empty_cache()\n",
                "  gc.collect()\n",
                "\n",
                "  augs = torch.nn.Sequential(\n",
                "    PadCrop(effective_length, randomize=True),\n",
                "    Stereo()\n",
                "  )\n",
                "\n",
                "  fp = recording_file_path if record_audio else file_path\n",
                "\n",
                "  audio_sample = load_to_device(fp, args.sample_rate)\n",
                "\n",
                "  audio_sample = augs(audio_sample).unsqueeze(0).repeat([batch_size, 1, 1])\n",
                "\n",
                "  print(\"Initial audio sample\")\n",
                "  plot_and_hear(audio_sample[0], args.sample_rate)\n",
                "  \n",
                "  generated = resample(model_fn, audio_sample, steps, sampler_type, noise_level=noise_level)\n",
                "\n",
                "  print(\"Regenerated audio samples\")\n",
                "  plot_and_hear(rearrange(generated, 'b d n -> d (b n)'), args.sample_rate)\n",
                "\n",
                "  for ix, gen_sample in enumerate(generated):\n",
                "    print(f'sample #{ix + 1}')\n",
                "    display(ipd.Audio(gen_sample.cpu(), rate=args.sample_rate))\n",
                "\n",
                "else:\n",
                "  print(\"Skipping section, uncheck 'skip_for_run_all' to enable\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "vW8N8GCCM-yT"
            },
            "source": [
                "# Interpolate between sounds\n",
                "Diffusion models allow for interpolation between inputs through a process of deterministic noising and denoising. \n",
                "\n",
                "By deterministically noising two audio files, interpolating between the results, and deterministically denoising them, we can can create new sounds \"between\" the audio files provided."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "l3Al3thgO5rb"
            },
            "outputs": [],
            "source": [
                "# Interpolation code taken and modified from CRASH\n",
                "def compute_interpolation_in_latent(latent1, latent2, lambd):\n",
                "    '''\n",
                "    Implementation of Spherical Linear Interpolation: https://en.wikipedia.org/wiki/Slerp\n",
                "    latent1: tensor of shape (2, n)\n",
                "    latent2: tensor of shape (2, n)\n",
                "    lambd: list of floats between 0 and 1 representing the parameter t of the Slerp\n",
                "    '''\n",
                "    device = latent1.device\n",
                "    lambd = torch.tensor(lambd)\n",
                "\n",
                "    assert(latent1.shape[0] == latent2.shape[0])\n",
                "\n",
                "    # get the number of channels\n",
                "    nc = latent1.shape[0]\n",
                "    interps = []\n",
                "    for channel in range(nc):\n",
                "    \n",
                "      cos_omega = latent1[channel]@latent2[channel] / \\\n",
                "          (torch.linalg.norm(latent1[channel])*torch.linalg.norm(latent2[channel]))\n",
                "      omega = torch.arccos(cos_omega).item()\n",
                "\n",
                "      a = torch.sin((1-lambd)*omega) / np.sin(omega)\n",
                "      b = torch.sin(lambd*omega) / np.sin(omega)\n",
                "      a = a.unsqueeze(1).to(device)\n",
                "      b = b.unsqueeze(1).to(device)\n",
                "      interps.append(a * latent1[channel] + b * latent2[channel])\n",
                "    return rearrange(torch.cat(interps), \"(c b) n -> b c n\", c=nc) \n",
                "\n",
                "#@markdown Enter the paths to two audio files to interpolate between (.wav or .flac)\n",
                "source_audio_path = \"\" #@param{type:\"string\"}\n",
                "target_audio_path = \"\" #@param{type:\"string\"}\n",
                "\n",
                "#@markdown Total number of steps (100 is a good start, can go lower for more speed/less quality)\n",
                "steps = 100#@param {type:\"number\"}\n",
                "\n",
                "#@markdown Number of interpolated samples\n",
                "n_interps = 12 #@param {type:\"number\"}\n",
                "\n",
                "#@markdown Multiplier on the default sample length from the model, allows for longer audio clips at the expense of VRAM\n",
                "sample_length_mult = 1#@param {type:\"number\"}\n",
                "\n",
                "#@markdown Check the box below to skip this section when running all cells\n",
                "skip_for_run_all = False #@param {type: \"boolean\"}\n",
                "\n",
                "effective_length = args.sample_size * sample_length_mult\n",
                "\n",
                "if not skip_for_run_all:\n",
                "\n",
                "  augs = torch.nn.Sequential(\n",
                "    PadCrop(effective_length, randomize=True),\n",
                "    Stereo()\n",
                "  )\n",
                "\n",
                "  if source_audio_path == \"\":\n",
                "    print(\"No file path provided for the source audio, please upload a file\")\n",
                "    uploaded = files.upload()\n",
                "    source_audio_path = list(uploaded.keys())[0]\n",
                "\n",
                "  audio_sample_1 = load_to_device(source_audio_path, args.sample_rate)\n",
                "\n",
                "  print(\"Source audio sample loaded\")\n",
                "\n",
                "  if target_audio_path == \"\":\n",
                "    print(\"No file path provided for the target audio, please upload a file\")\n",
                "    uploaded = files.upload()\n",
                "    target_audio_path = list(uploaded.keys())[0]\n",
                "\n",
                "  audio_sample_2 = load_to_device(target_audio_path, args.sample_rate)\n",
                "\n",
                "  print(\"Target audio sample loaded\")\n",
                "\n",
                "  audio_samples = augs(audio_sample_1).unsqueeze(0).repeat([2, 1, 1])\n",
                "  audio_samples[1] = augs(audio_sample_2)\n",
                "\n",
                "  print(\"Initial audio samples\")\n",
                "  plot_and_hear(audio_samples[0], args.sample_rate)\n",
                "  plot_and_hear(audio_samples[1], args.sample_rate)\n",
                "\n",
                "  reversed = reverse_sample(model_fn, audio_samples, steps)\n",
                "\n",
                "  latent_series = compute_interpolation_in_latent(reversed[0], reversed[1], [k/n_interps for k in range(n_interps + 2)])\n",
                "\n",
                "  generated = sample(model_fn, latent_series, steps) \n",
                "  \n",
                "  #sampling.iplms_sample(, latent_series, step_list.flip(0)[:-1], {})\n",
                "\n",
                "  # Put the demos together\n",
                "  generated_all = rearrange(generated, 'b d n -> d (b n)')\n",
                "\n",
                "  print(\"Full interpolation\")\n",
                "  plot_and_hear(generated_all, args.sample_rate)\n",
                "  for ix, gen_sample in enumerate(generated):\n",
                "    print(f'sample #{ix + 1}')\n",
                "    display(ipd.Audio(gen_sample.cpu(), rate=args.sample_rate))\n",
                "else:\n",
                "  print(\"Skipping section, uncheck 'skip_for_run_all' to enable\") "
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "provenance": []
        },
        "gpuClass": "standard",
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
        },
        "vscode": {
            "interpreter": {
                "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
